{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UgvZ_H8MGTmz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "from statistics import mean\n",
        "from PIL import Image\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Base directory (adjust as needed)\n",
        "base_dir = \"/content/drive/MyDrive/projet_collembolles\"\n",
        "\n",
        "# Paths for CSVs and image folders\n",
        "AGREED_BOXES_CSV = os.path.join(base_dir, \"agreed_boxes.csv\")\n",
        "BACKGROUND_BOXES_CSV = os.path.join(base_dir, \"random_background_boxes.csv\")\n",
        "IMAGES_FOLDER = os.path.join(base_dir, \"data\")       # Labeled images folder (uncropped)\n",
        "TEST_FOLDER = os.path.join(base_dir, \"datatest\")       # Test images folder (already cropped)\n",
        "\n",
        "print(\"AGREED_BOXES_CSV:\", AGREED_BOXES_CSV)\n",
        "print(\"BACKGROUND_BOXES_CSV:\", BACKGROUND_BOXES_CSV)\n",
        "print(\"IMAGES_FOLDER:\", IMAGES_FOLDER)\n",
        "print(\"TEST_FOLDER:\", TEST_FOLDER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0TknGOJGoo0",
        "outputId": "8d7713e8-b0cd-412c-88e7-e566bd2a2dbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "AGREED_BOXES_CSV: /content/drive/MyDrive/projet_collembolles/agreed_boxes.csv\n",
            "BACKGROUND_BOXES_CSV: /content/drive/MyDrive/projet_collembolles/random_background_boxes.csv\n",
            "IMAGES_FOLDER: /content/drive/MyDrive/projet_collembolles/data\n",
            "TEST_FOLDER: /content/drive/MyDrive/projet_collembolles/datatest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzYZk_SjGTm0"
      },
      "source": [
        "## Nombre d'avis en commun pour créé label :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Kk6TAOPgGTm1"
      },
      "outputs": [],
      "source": [
        "min_avis = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoaJJQ5xGTm1"
      },
      "source": [
        "## Créé fonction csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "or0k9Kp0GTm1"
      },
      "outputs": [],
      "source": [
        "# Le dossier où il y a les images, pour récupérer chaque txt\n",
        "def info_data(txt_folder):\n",
        "    data = []\n",
        "    for txt_file in os.listdir(txt_folder):\n",
        "        if txt_file.endswith('.txt'):\n",
        "            with open(os.path.join(txt_folder, txt_file), 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                for line in lines:\n",
        "                    parts = line.strip().split()\n",
        "                    avis = parts[0]\n",
        "                    xc = parts[-4]\n",
        "                    yc = parts[-3]\n",
        "                    w = parts[-2]\n",
        "                    h = parts[-1]\n",
        "                    other = parts[1:-4]\n",
        "                    file_id = txt_file.replace('.txt', '')\n",
        "                    data.append([file_id, avis] + [xc, yc, w, h] + other)\n",
        "    columns = ['id', 'avis'] + ['xc', 'yc', 'w', 'h'] + ['classe','year'] + [f'info{i+2}' for i in range(4)]\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "    df.to_csv('info_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0VHMV_mGTm1"
      },
      "source": [
        "## CLasse python image et background"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# On définit la graine pour assurer la reproductibilité\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "WNzOOmysHxqP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XgsrRpGMGTm1"
      },
      "outputs": [],
      "source": [
        "class BackgroundDataset(Dataset):\n",
        "    def __init__(self, images_dir, csv_file, transform=None, num_samples=1000):\n",
        "        \"\"\"\n",
        "        Dataset qui génère des images de fond (classe 8) en évitant les bounding boxes existantes.\n",
        "\n",
        "        Args:\n",
        "            images_dir (string): Dossier contenant les images.\n",
        "            csv_file (string): Chemin vers le fichier CSV contenant les informations.\n",
        "            transform (callable, optional): Transformations à appliquer aux images.\n",
        "            num_samples (int): Nombre de samples de background à générer.\n",
        "        \"\"\"\n",
        "        self.images_dir = images_dir\n",
        "        self.transform = transform\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "        # Charger les informations du dataset original\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.boxes = self._load_boxes()\n",
        "        self.image_ids = list(self.boxes.keys())  # Liste des images disponibles\n",
        "\n",
        "    def _load_boxes(self):\n",
        "        \"\"\"\n",
        "        Charge toutes les bounding boxes sous forme de dictionnaire {image_id: [bbox_list]}.\n",
        "        \"\"\"\n",
        "        boxes = {}\n",
        "        for _, row in self.data.iterrows():\n",
        "            image_id = str(row['id'])\n",
        "            bbox = [row['xc'], row['yc'], row['w'], row['h']]\n",
        "            if image_id not in boxes:\n",
        "                boxes[image_id] = []\n",
        "            boxes[image_id].append(bbox)\n",
        "        return boxes\n",
        "\n",
        "    def _get_random_background_patch(self, image, bboxes):\n",
        "        \"\"\"\n",
        "        Extrait une zone aléatoire de l'image en évitant les bounding boxes existantes.\n",
        "        \"\"\"\n",
        "        width, height = image.size\n",
        "        patch_size = 822  # Taille fixe du patch\n",
        "\n",
        "        for _ in range(10):  # Essayer 10 fois de trouver une zone correcte\n",
        "            x = random.randint(0, width - patch_size)\n",
        "            y = random.randint(0, height - patch_size)\n",
        "\n",
        "            # Vérifier si la zone chevauche une bounding box\n",
        "            overlaps = False\n",
        "            for (xc, yc, w, h) in bboxes:\n",
        "                x_min = int((xc - w / 2) * width)\n",
        "                x_max = int((xc + w / 2) * width)\n",
        "                y_min = int((yc - h / 2) * height)\n",
        "                y_max = int((yc + h / 2) * height)\n",
        "\n",
        "                if not (x_max < x or x_min > x + patch_size or y_max < y or y_min > y + patch_size):\n",
        "                    overlaps = True\n",
        "                    break\n",
        "\n",
        "            if not overlaps:\n",
        "                return image.crop((x, y, x + patch_size, y + patch_size))\n",
        "\n",
        "        # Si aucune zone correcte n'a été trouvée après 10 essais, prendre une zone au hasard\n",
        "        return image.crop((0, 0, patch_size, patch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retourne un patch de fond et le label 8.\n",
        "        \"\"\"\n",
        "        image_id = random.choice(self.image_ids)\n",
        "        image_path = os.path.join(self.images_dir, f\"{image_id}.jpg\")\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        bboxes = self.boxes[image_id]\n",
        "        background_patch = self._get_random_background_patch(image, bboxes)\n",
        "\n",
        "        if self.transform:\n",
        "            background_patch = self.transform(background_patch)\n",
        "\n",
        "        label = torch.tensor(8, dtype=torch.long)\n",
        "\n",
        "        return background_patch, label\n",
        "\n",
        "\n",
        "class ImageBoundingBoxDataset(Dataset):\n",
        "    def __init__(self, images_dir, csv_file, transform=None, split=\"train\", seed=42):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images_dir (string): Dossier contenant les images.\n",
        "            csv_file (string): Chemin vers le fichier CSV contenant les informations.\n",
        "            transform (callable, optional): Transformations à appliquer aux images.\n",
        "            split (string): \"train\", \"val\" ou \"test\" pour choisir le dataset.\n",
        "            seed (int): Pour rendre la répartition fixe.\n",
        "        \"\"\"\n",
        "        self.images_dir = images_dir\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "        self.seed = seed\n",
        "\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.boxes = self._create_boxes_list()\n",
        "\n",
        "        self._split_data()\n",
        "\n",
        "        self.class_counts = self.count_classes()\n",
        "\n",
        "    def _create_boxes_list(self):\n",
        "        \"\"\"\n",
        "        Crée une liste de dictionnaires où chaque boîte est une entrée unique.\n",
        "        \"\"\"\n",
        "        boxes = []\n",
        "        for _, row in self.data.iterrows():\n",
        "            image_id = str(row['id'])\n",
        "            bbox = [row['xc'], row['yc'], row['w'], row['h']]\n",
        "            avis = row['avis']\n",
        "            label = self.avis_majoritaire(avis)\n",
        "            if label not in [None, 8]:\n",
        "                boxes.append({\n",
        "                    'image_id': image_id,\n",
        "                    'bbox': bbox,\n",
        "                    'label': label\n",
        "                })\n",
        "        return boxes\n",
        "\n",
        "    def _split_data(self):\n",
        "        \"\"\"\n",
        "        Effectue le split de l'ensemble de données en train, validation et test,\n",
        "        et choisit le split actif.\n",
        "        \"\"\"\n",
        "        labels = [box[\"label\"] for box in self.boxes]\n",
        "        train_data, test_data = train_test_split(self.boxes, test_size=0.4, random_state=self.seed, stratify=labels)\n",
        "\n",
        "        # Choisir le dataset basé sur le split demandé\n",
        "        if self.split == \"train\":\n",
        "            self.data_split = train_data\n",
        "        elif self.split == \"test\":\n",
        "            self.data_split = test_data\n",
        "        else:\n",
        "            raise ValueError(\"Split must be one of ['train', 'val', 'test']\")\n",
        "\n",
        "    def count_classes(self):\n",
        "        \"\"\"\n",
        "        Compte le nombre d'instances pour chaque classe dans le dataset actuel.\n",
        "        \"\"\"\n",
        "        class_counts = defaultdict(int)\n",
        "        for annotation in self.data_split:\n",
        "            label = annotation['label']\n",
        "            class_counts[label] += 1\n",
        "        return dict(class_counts)\n",
        "\n",
        "\n",
        "    def avis_majoritaire(self, avis, min_count=min_avis):\n",
        "        \"\"\"Calcule l'avis majoritaire uniquement s'il dépasse un seuil minimal.\"\"\"\n",
        "        parts = avis.split('_')\n",
        "        count = Counter(parts)\n",
        "\n",
        "        majoritaire, occurrences = max(count.items(), key=lambda x: x[1])\n",
        "\n",
        "        if occurrences >= min_count:\n",
        "            return int(majoritaire)\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_split)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retourne une image découpée selon la boîte englobante et son label.\n",
        "        \"\"\"\n",
        "        annotation = self.data_split[idx]\n",
        "        bbox = annotation['bbox']\n",
        "        label = annotation['label']\n",
        "\n",
        "        image_path = os.path.join(self.images_dir, f\"{annotation['image_id']}.jpg\")\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        xc, yc, w, h = bbox\n",
        "        x_min = int((xc - w / 2) * image.width)\n",
        "        x_max = int((xc + w / 2) * image.width)\n",
        "        y_min = int((yc - h / 2) * image.height)\n",
        "        y_max = int((yc + h / 2) * image.height)\n",
        "\n",
        "        cropped_image = image.crop((x_min, y_min, x_max, y_max))\n",
        "\n",
        "        if self.transform:\n",
        "            cropped_image = self.transform(cropped_image)\n",
        "\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return cropped_image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Image_label_Dataset(Dataset):\n",
        "    def __init__(self, images_dir, csv_file, fallback_csv, transform=None, split=\"train\", seed=42):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images_dir (string): Dossier contenant les images.\n",
        "            csv_file (string): Chemin vers le fichier CSV contenant les informations principales.\n",
        "            fallback_csv (string): Chemin vers le fichier CSV contenant image_id + bbox + final_label.\n",
        "            transform (callable, optional): Transformations à appliquer aux images.\n",
        "            split (string): \"train\" ou \"test\" pour choisir le dataset.\n",
        "            seed (int): Pour rendre la répartition fixe.\n",
        "        \"\"\"\n",
        "\n",
        "        self.images_dir = images_dir\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "        self.seed = seed\n",
        "\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.fallback_data = pd.read_csv(fallback_csv)\n",
        "        self.boxes = self._create_boxes_list()\n",
        "\n",
        "        self._split_data()\n",
        "        self.class_counts = self.count_classes()\n",
        "\n",
        "    def _create_boxes_list(self):\n",
        "        \"\"\"\n",
        "        Crée une liste de dictionnaires où chaque boîte est une entrée unique.\n",
        "        \"\"\"\n",
        "        boxes = []\n",
        "        for _, row in self.data.iterrows():\n",
        "            image_id = str(row['id'])\n",
        "            bbox = [row['xc'], row['yc'], row['w'], row['h']]\n",
        "            avis = row['avis']\n",
        "            label = self.avis_majoritaire(avis)\n",
        "\n",
        "            if label is None:\n",
        "                label = self.get_fallback_label(image_id, bbox)\n",
        "\n",
        "            if label not in [None, 8]:\n",
        "                boxes.append({\n",
        "                    'image_id': image_id,\n",
        "                    'bbox': bbox,\n",
        "                    'label': label\n",
        "                })\n",
        "        return boxes\n",
        "\n",
        "    def get_fallback_label(self, image_id, bbox):\n",
        "        \"\"\"\n",
        "        Cherche le label dans le fichier CSV de fallback si avis_majoritaire retourne None.\n",
        "        \"\"\"\n",
        "        # Convertir la chaîne de caractères du 'bbox' du fichier fallback en tuple\n",
        "        self.fallback_data['bbox_tuple'] = self.fallback_data['bbox'].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "        # Arrondir le bbox à 5 décimales pour correspondre au format du fichier fallback\n",
        "        bbox_round = tuple(round(val, 5) for val in bbox)\n",
        "\n",
        "        # Recherche d'une correspondance\n",
        "        match = self.fallback_data[(self.fallback_data['idx'] == image_id) &\n",
        "                                    (self.fallback_data['bbox_tuple'] == bbox_round)]\n",
        "\n",
        "        if not match.empty:\n",
        "            return int(match.iloc[0]['final_label'])\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _split_data(self):\n",
        "        \"\"\"\n",
        "        Effectue le split de l'ensemble de données en train et test.\n",
        "        \"\"\"\n",
        "        labels = [box[\"label\"] for box in self.boxes]\n",
        "        train_data, test_data = train_test_split(self.boxes, test_size=0.4, random_state=self.seed, stratify=labels)\n",
        "\n",
        "        if self.split == \"train\":\n",
        "            self.data_split = train_data\n",
        "        elif self.split == \"test\":\n",
        "            self.data_split = test_data\n",
        "        else:\n",
        "            raise ValueError(\"Split must be one of ['train', 'test']\")\n",
        "\n",
        "    def count_classes(self):\n",
        "        \"\"\"\n",
        "        Compte le nombre d'instances pour chaque classe dans le dataset actuel.\n",
        "        \"\"\"\n",
        "        class_counts = defaultdict(int)\n",
        "        for annotation in self.data_split:\n",
        "            label = annotation['label']\n",
        "            class_counts[label] += 1\n",
        "        return dict(class_counts)\n",
        "\n",
        "    def avis_majoritaire(self, avis, min_count=4):\n",
        "        \"\"\"Calcule l'avis majoritaire uniquement s'il dépasse un seuil minimal.\"\"\"\n",
        "        parts = avis.split('_')\n",
        "        count = Counter(parts)\n",
        "\n",
        "        majoritaire, occurrences = max(count.items(), key=lambda x: x[1])\n",
        "\n",
        "        if occurrences >= min_count:\n",
        "            return int(majoritaire)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_split)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retourne une image découpée selon la boîte englobante et son label.\n",
        "        \"\"\"\n",
        "        annotation = self.data_split[idx]\n",
        "        bbox = annotation['bbox']\n",
        "        label = annotation['label']\n",
        "\n",
        "        image_path = os.path.join(self.images_dir, f\"{annotation['image_id']}.jpg\")\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        xc, yc, w, h = bbox\n",
        "        x_min = int((xc - w / 2) * image.width)\n",
        "        x_max = int((xc + w / 2) * image.width)\n",
        "        y_min = int((yc - h / 2) * image.height)\n",
        "        y_max = int((yc + h / 2) * image.height)\n",
        "\n",
        "        cropped_image = image.crop((x_min, y_min, x_max, y_max))\n",
        "\n",
        "        if self.transform:\n",
        "            cropped_image = self.transform(cropped_image)\n",
        "\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return cropped_image, label\n"
      ],
      "metadata": {
        "id": "eOVjjDJVD1Cp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCvzod99GTm2"
      },
      "source": [
        "## Création du dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from statistics import mean\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torchvision import transforms, models"
      ],
      "metadata": {
        "id": "m9QJ2nXmHglT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4gpyTSSKGTm2"
      },
      "outputs": [],
      "source": [
        "img_dir = IMAGES_FOLDER\n",
        "csv_file = \"/content/drive/MyDrive/projet_collembolles/info_data.csv\"  # chemin complet vers le fichier CSV\n",
        "label_csv = \"/content/drive/MyDrive/projet_collembolles/final_predict_logit-3.csv\"  # chemin complet vers le fichier CSV\n",
        "\n",
        "image_size = 224\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-4\n",
        "num_classes = 9\n",
        "EPOCHS = 20\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.Resize((image_size, image_size), antialias=True),\n",
        "    transforms.ToTensor(),\n",
        "    # Optionally, add normalization\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset_label = Image_label_Dataset(images_dir=img_dir, csv_file=csv_file, fallback_csv = label_csv, transform=transform, split='train')\n",
        "test_dataset_label = Image_label_Dataset(images_dir=img_dir, csv_file=csv_file, fallback_csv = label_csv, transform=transform, split='test')\n",
        "\n",
        "train_background = BackgroundDataset(img_dir, csv_file, transform=transform, num_samples=int(mean(train_dataset_label.class_counts.values())))\n",
        "test_background = BackgroundDataset(img_dir, csv_file, transform=transform, num_samples=int(mean(test_dataset_label.class_counts.values())))\n",
        "\n",
        "train_data_background = torch.utils.data.ConcatDataset([train_dataset_label, train_background])\n",
        "test_data_background = torch.utils.data.ConcatDataset([test_dataset_label, test_background])\n",
        "\n",
        "full_data_background = torch.utils.data.ConcatDataset([train_data_background, test_data_background])\n",
        "full_dataset = torch.utils.data.ConcatDataset([train_dataset_label, test_dataset_label])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Taille de train_dataset_label :\", len(train_dataset_label))\n",
        "print(\"Taille de test_dataset_label :\", len(test_dataset_label))\n",
        "print(\"Taille de train_background :\", len(train_background))\n",
        "print(\"Taille de test_background :\", len(test_background))\n",
        "print(\"Taille de train_data_background :\", len(train_data_background))\n",
        "print(\"Taille de test_data_background :\", len(test_data_background))\n",
        "print(\"Taille de full_data_background :\", len(full_data_background))\n",
        "print(\"Taille de full_dataset :\", len(full_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV57YazkFG4F",
        "outputId": "88c91786-75ae-484b-9929-56ae6be8bc29"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille de train_dataset_label : 837\n",
            "Taille de test_dataset_label : 559\n",
            "Taille de train_background : 104\n",
            "Taille de test_background : 69\n",
            "Taille de train_data_background : 941\n",
            "Taille de test_data_background : 628\n",
            "Taille de full_data_background : 1569\n",
            "Taille de full_dataset : 1396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data_background, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_data_background, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "n_iyuCvd-jdO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Chargement et adaptation du modèle ViT pré-entraîné\n"
      ],
      "metadata": {
        "id": "wbYJKPYy7ywR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "cMMMLF40ImQQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.vit_b_16(pretrained=True)\n",
        "# Remplacer la dernière couche (head) afin d'avoir num_classes sorties\n",
        "model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
        "\n",
        "# Utilisation du GPU si disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iexNh4WHRpi",
        "outputId": "0449fd18-f498-43de-c67b-6466a07915c1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                       factor=0.5, patience=3, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al_YAlWUN2eh",
        "outputId": "d3f98247-f989-41a8-9c66-f6c391ded044"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_train_labels = []\n",
        "    all_train_preds = []\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        all_train_labels.extend(labels.cpu().numpy())\n",
        "        all_train_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    train_loss /= total\n",
        "    train_accuracy = correct / total\n",
        "    train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')\n",
        "\n",
        "    # Évaluation\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    all_test_labels = []\n",
        "    all_test_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "            all_test_labels.extend(labels.cpu().numpy())\n",
        "            all_test_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    test_loss /= total_test\n",
        "    test_accuracy = correct_test / total_test\n",
        "    test_f1 = f1_score(all_test_labels, all_test_preds, average='macro')\n",
        "\n",
        "    scheduler.step(test_loss)\n",
        "\n",
        "    print(f\"\\nEpoch [{epoch+1}/{EPOCHS}]\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Train F1: {train_f1:.4f}\")\n",
        "    print(\"Train classification report :\")\n",
        "    print(classification_report(all_train_labels, all_train_preds, digits=4))\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}, Test F1: {test_f1:.4f}\")\n",
        "    print(\"Test classification report :\")\n",
        "    print(classification_report(all_test_labels, all_test_preds, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jB3i7ZnN2c5",
        "outputId": "3bbc7e9a-4a99-46ac-8ad2-8ef29a5a554b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [1/20]\n",
            "Train Loss: 1.5313, Train Acc: 0.4516, Train F1: 0.3228\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4286    0.7213    0.5377       287\n",
            "           1     0.4737    0.5870    0.5243       138\n",
            "           2     0.3077    0.0533    0.0909        75\n",
            "           3     0.4815    0.4062    0.4407        64\n",
            "           4     0.1613    0.1000    0.1235        50\n",
            "           5     0.3125    0.1724    0.2222        87\n",
            "           6     0.3810    0.1231    0.1860        65\n",
            "           7     0.3750    0.0423    0.0759        71\n",
            "           8     0.6786    0.7308    0.7037       104\n",
            "\n",
            "    accuracy                         0.4516       941\n",
            "   macro avg     0.4000    0.3263    0.3228       941\n",
            "weighted avg     0.4245    0.4516    0.4015       941\n",
            "\n",
            "Test Loss: 1.5771, Test Acc: 0.4283, Test F1: 0.3491\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3526    0.6387    0.4544       191\n",
            "           1     0.4000    0.0217    0.0412        92\n",
            "           2     0.3194    0.4510    0.3740        51\n",
            "           3     0.6923    0.6279    0.6585        43\n",
            "           4     0.2121    0.2121    0.2121        33\n",
            "           5     0.0000    0.0000    0.0000        59\n",
            "           6     0.3833    0.5349    0.4466        43\n",
            "           7     0.5000    0.0213    0.0408        47\n",
            "           8     0.9014    0.9275    0.9143        69\n",
            "\n",
            "    accuracy                         0.4283       628\n",
            "   macro avg     0.4179    0.3817    0.3491       628\n",
            "weighted avg     0.4130    0.4283    0.3649       628\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [2/20]\n",
            "Train Loss: 1.0043, Train Acc: 0.6206, Train F1: 0.5657\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5333    0.7247    0.6145       287\n",
            "           1     0.7482    0.7536    0.7509       138\n",
            "           2     0.4182    0.3067    0.3538        75\n",
            "           3     0.7627    0.7031    0.7317        64\n",
            "           4     0.3235    0.2200    0.2619        50\n",
            "           5     0.5733    0.4943    0.5309        87\n",
            "           6     0.6667    0.5538    0.6050        65\n",
            "           7     0.5172    0.2113    0.3000        71\n",
            "           8     0.9340    0.9519    0.9429       104\n",
            "\n",
            "    accuracy                         0.6206       941\n",
            "   macro avg     0.6086    0.5466    0.5657       941\n",
            "weighted avg     0.6161    0.6206    0.6071       941\n",
            "\n",
            "Test Loss: 1.0778, Test Acc: 0.6051, Test F1: 0.5422\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6051    0.4974    0.5460       191\n",
            "           1     0.5714    0.8696    0.6897        92\n",
            "           2     0.5789    0.4314    0.4944        51\n",
            "           3     0.8571    0.6977    0.7692        43\n",
            "           4     0.0000    0.0000    0.0000        33\n",
            "           5     0.4842    0.7797    0.5974        59\n",
            "           6     0.4267    0.7442    0.5424        43\n",
            "           7     0.5882    0.2128    0.3125        47\n",
            "           8     0.9155    0.9420    0.9286        69\n",
            "\n",
            "    accuracy                         0.6051       628\n",
            "   macro avg     0.5586    0.5750    0.5422       628\n",
            "weighted avg     0.5928    0.6051    0.5786       628\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [3/20]\n",
            "Train Loss: 0.8685, Train Acc: 0.6674, Train F1: 0.6132\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6140    0.7317    0.6677       287\n",
            "           1     0.7763    0.8551    0.8138       138\n",
            "           2     0.5000    0.3600    0.4186        75\n",
            "           3     0.8361    0.7969    0.8160        64\n",
            "           4     0.2581    0.1600    0.1975        50\n",
            "           5     0.6395    0.6322    0.6358        87\n",
            "           6     0.5915    0.6462    0.6176        65\n",
            "           7     0.6279    0.3803    0.4737        71\n",
            "           8     0.8911    0.8654    0.8780       104\n",
            "\n",
            "    accuracy                         0.6674       941\n",
            "   macro avg     0.6372    0.6031    0.6132       941\n",
            "weighted avg     0.6574    0.6674    0.6566       941\n",
            "\n",
            "Test Loss: 0.8149, Test Acc: 0.6943, Test F1: 0.6285\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5846    0.7958    0.6741       191\n",
            "           1     0.8649    0.6957    0.7711        92\n",
            "           2     0.7778    0.4118    0.5385        51\n",
            "           3     0.7500    0.9070    0.8211        43\n",
            "           4     0.1667    0.0303    0.0513        33\n",
            "           5     0.6897    0.6780    0.6838        59\n",
            "           6     0.6786    0.8837    0.7677        43\n",
            "           7     0.6364    0.2979    0.4058        47\n",
            "           8     0.9178    0.9710    0.9437        69\n",
            "\n",
            "    accuracy                         0.6943       628\n",
            "   macro avg     0.6740    0.6301    0.6285       628\n",
            "weighted avg     0.6875    0.6943    0.6715       628\n",
            "\n",
            "\n",
            "Epoch [4/20]\n",
            "Train Loss: 0.6847, Train Acc: 0.7343, Train F1: 0.7022\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6455    0.7422    0.6904       287\n",
            "           1     0.8803    0.9058    0.8929       138\n",
            "           2     0.6232    0.5733    0.5972        75\n",
            "           3     0.9048    0.8906    0.8976        64\n",
            "           4     0.3793    0.2200    0.2785        50\n",
            "           5     0.7024    0.6782    0.6901        87\n",
            "           6     0.7286    0.7846    0.7556        65\n",
            "           7     0.7059    0.5070    0.5902        71\n",
            "           8     0.9320    0.9231    0.9275       104\n",
            "\n",
            "    accuracy                         0.7343       941\n",
            "   macro avg     0.7224    0.6916    0.7022       941\n",
            "weighted avg     0.7288    0.7343    0.7280       941\n",
            "\n",
            "Test Loss: 0.8993, Test Acc: 0.6545, Test F1: 0.6503\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6646    0.5707    0.6141       191\n",
            "           1     0.7831    0.7065    0.7429        92\n",
            "           2     0.5152    0.6667    0.5812        51\n",
            "           3     0.8421    0.7442    0.7901        43\n",
            "           4     0.3623    0.7576    0.4902        33\n",
            "           5     0.7429    0.4407    0.5532        59\n",
            "           6     0.6154    0.9302    0.7407        43\n",
            "           7     0.4130    0.4043    0.4086        47\n",
            "           8     0.9839    0.8841    0.9313        69\n",
            "\n",
            "    accuracy                         0.6545       628\n",
            "   macro avg     0.6581    0.6783    0.6503       628\n",
            "weighted avg     0.6863    0.6545    0.6582       628\n",
            "\n",
            "\n",
            "Epoch [5/20]\n",
            "Train Loss: 0.5525, Train Acc: 0.7928, Train F1: 0.7800\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7296    0.7805    0.7542       287\n",
            "           1     0.9007    0.9203    0.9104       138\n",
            "           2     0.7246    0.6667    0.6944        75\n",
            "           3     0.9032    0.8750    0.8889        64\n",
            "           4     0.5833    0.5600    0.5714        50\n",
            "           5     0.7882    0.7701    0.7791        87\n",
            "           6     0.8209    0.8462    0.8333        65\n",
            "           7     0.6935    0.6056    0.6466        71\n",
            "           8     0.9600    0.9231    0.9412       104\n",
            "\n",
            "    accuracy                         0.7928       941\n",
            "   macro avg     0.7894    0.7719    0.7800       941\n",
            "weighted avg     0.7928    0.7928    0.7921       941\n",
            "\n",
            "Test Loss: 0.8735, Test Acc: 0.6975, Test F1: 0.6537\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6047    0.8010    0.6892       191\n",
            "           1     0.7791    0.7283    0.7528        92\n",
            "           2     0.6071    0.6667    0.6355        51\n",
            "           3     0.9394    0.7209    0.8158        43\n",
            "           4     0.5556    0.1515    0.2381        33\n",
            "           5     0.6615    0.7288    0.6935        59\n",
            "           6     0.8000    0.4651    0.5882        43\n",
            "           7     0.6286    0.4681    0.5366        47\n",
            "           8     0.9545    0.9130    0.9333        69\n",
            "\n",
            "    accuracy                         0.6975       628\n",
            "   macro avg     0.7256    0.6271    0.6537       628\n",
            "weighted avg     0.7097    0.6975    0.6880       628\n",
            "\n",
            "\n",
            "Epoch [6/20]\n",
            "Train Loss: 0.5379, Train Acc: 0.8034, Train F1: 0.7804\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7703    0.7944    0.7822       287\n",
            "           1     0.8794    0.8986    0.8889       138\n",
            "           2     0.7761    0.6933    0.7324        75\n",
            "           3     0.8906    0.8906    0.8906        64\n",
            "           4     0.5349    0.4600    0.4946        50\n",
            "           5     0.8605    0.8506    0.8555        87\n",
            "           6     0.7778    0.8615    0.8175        65\n",
            "           7     0.5942    0.5775    0.5857        71\n",
            "           8     0.9806    0.9712    0.9758       104\n",
            "\n",
            "    accuracy                         0.8034       941\n",
            "   macro avg     0.7849    0.7775    0.7804       941\n",
            "weighted avg     0.8012    0.8034    0.8018       941\n",
            "\n",
            "Test Loss: 0.9889, Test Acc: 0.6831, Test F1: 0.6502\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5859    0.7853    0.6711       191\n",
            "           1     0.8219    0.6522    0.7273        92\n",
            "           2     0.7097    0.4314    0.5366        51\n",
            "           3     1.0000    0.3953    0.5667        43\n",
            "           4     0.4583    0.3333    0.3860        33\n",
            "           5     0.6400    0.8136    0.7164        59\n",
            "           6     0.8500    0.7907    0.8193        43\n",
            "           7     0.5366    0.4681    0.5000        47\n",
            "           8     0.9155    0.9420    0.9286        69\n",
            "\n",
            "    accuracy                         0.6831       628\n",
            "   macro avg     0.7242    0.6235    0.6502       628\n",
            "weighted avg     0.7079    0.6831    0.6762       628\n",
            "\n",
            "\n",
            "Epoch [7/20]\n",
            "Train Loss: 0.4822, Train Acc: 0.8278, Train F1: 0.8144\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7796    0.8258    0.8020       287\n",
            "           1     0.9078    0.9275    0.9176       138\n",
            "           2     0.7538    0.6533    0.7000        75\n",
            "           3     0.8730    0.8594    0.8661        64\n",
            "           4     0.6038    0.6400    0.6214        50\n",
            "           5     0.8765    0.8161    0.8452        87\n",
            "           6     0.8939    0.9077    0.9008        65\n",
            "           7     0.7424    0.6901    0.7153        71\n",
            "           8     0.9706    0.9519    0.9612       104\n",
            "\n",
            "    accuracy                         0.8278       941\n",
            "   macro avg     0.8224    0.8080    0.8144       941\n",
            "weighted avg     0.8285    0.8278    0.8275       941\n",
            "\n",
            "Test Loss: 0.8930, Test Acc: 0.6863, Test F1: 0.6477\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6993    0.5602    0.6221       191\n",
            "           1     0.7379    0.8261    0.7795        92\n",
            "           2     0.4433    0.8431    0.5811        51\n",
            "           3     0.9062    0.6744    0.7733        43\n",
            "           4     0.3333    0.0606    0.1026        33\n",
            "           5     0.6154    0.9492    0.7467        59\n",
            "           6     0.7273    0.7442    0.7356        43\n",
            "           7     0.6053    0.4894    0.5412        47\n",
            "           8     0.9844    0.9130    0.9474        69\n",
            "\n",
            "    accuracy                         0.6863       628\n",
            "   macro avg     0.6725    0.6734    0.6477       628\n",
            "weighted avg     0.6974    0.6863    0.6740       628\n",
            "\n",
            "\n",
            "Epoch [8/20]\n",
            "Train Loss: 0.2529, Train Acc: 0.9097, Train F1: 0.9022\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8961    0.8711    0.8834       287\n",
            "           1     0.9500    0.9638    0.9568       138\n",
            "           2     0.8272    0.8933    0.8590        75\n",
            "           3     0.9844    0.9844    0.9844        64\n",
            "           4     0.7234    0.6800    0.7010        50\n",
            "           5     0.9551    0.9770    0.9659        87\n",
            "           6     0.9275    0.9846    0.9552        65\n",
            "           7     0.8406    0.8169    0.8286        71\n",
            "           8     0.9903    0.9808    0.9855       104\n",
            "\n",
            "    accuracy                         0.9097       941\n",
            "   macro avg     0.8994    0.9058    0.9022       941\n",
            "weighted avg     0.9092    0.9097    0.9091       941\n",
            "\n",
            "Test Loss: 0.7677, Test Acc: 0.7516, Test F1: 0.7300\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7225    0.7225    0.7225       191\n",
            "           1     0.8367    0.8913    0.8632        92\n",
            "           2     0.9091    0.3922    0.5479        51\n",
            "           3     0.9048    0.8837    0.8941        43\n",
            "           4     0.3673    0.5455    0.4390        33\n",
            "           5     0.8571    0.7119    0.7778        59\n",
            "           6     0.7755    0.8837    0.8261        43\n",
            "           7     0.5000    0.5745    0.5347        47\n",
            "           8     0.9324    1.0000    0.9650        69\n",
            "\n",
            "    accuracy                         0.7516       628\n",
            "   macro avg     0.7562    0.7339    0.7300       628\n",
            "weighted avg     0.7709    0.7516    0.7507       628\n",
            "\n",
            "\n",
            "Epoch [9/20]\n",
            "Train Loss: 0.1675, Train Acc: 0.9458, Train F1: 0.9411\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9201    0.9233    0.9217       287\n",
            "           1     0.9857    1.0000    0.9928       138\n",
            "           2     0.9437    0.8933    0.9178        75\n",
            "           3     1.0000    1.0000    1.0000        64\n",
            "           4     0.7500    0.7800    0.7647        50\n",
            "           5     1.0000    0.9655    0.9825        87\n",
            "           6     0.9403    0.9692    0.9545        65\n",
            "           7     0.9571    0.9437    0.9504        71\n",
            "           8     0.9810    0.9904    0.9856       104\n",
            "\n",
            "    accuracy                         0.9458       941\n",
            "   macro avg     0.9420    0.9406    0.9411       941\n",
            "weighted avg     0.9463    0.9458    0.9459       941\n",
            "\n",
            "Test Loss: 0.6729, Test Acc: 0.7850, Test F1: 0.7761\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7300    0.7644    0.7468       191\n",
            "           1     0.8667    0.8478    0.8571        92\n",
            "           2     0.7826    0.7059    0.7423        51\n",
            "           3     0.9024    0.8605    0.8810        43\n",
            "           4     0.4222    0.5758    0.4872        33\n",
            "           5     0.8868    0.7966    0.8393        59\n",
            "           6     0.8261    0.8837    0.8539        43\n",
            "           7     0.6429    0.5745    0.6067        47\n",
            "           8     1.0000    0.9420    0.9701        69\n",
            "\n",
            "    accuracy                         0.7850       628\n",
            "   macro avg     0.7844    0.7724    0.7761       628\n",
            "weighted avg     0.7944    0.7850    0.7882       628\n",
            "\n",
            "\n",
            "Epoch [10/20]\n",
            "Train Loss: 0.1556, Train Acc: 0.9437, Train F1: 0.9317\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9406    0.9373    0.9389       287\n",
            "           1     0.9857    1.0000    0.9928       138\n",
            "           2     0.8873    0.8400    0.8630        75\n",
            "           3     0.9844    0.9844    0.9844        64\n",
            "           4     0.7273    0.8000    0.7619        50\n",
            "           5     1.0000    0.9885    0.9942        87\n",
            "           6     0.9091    0.9231    0.9160        65\n",
            "           7     0.9437    0.9437    0.9437        71\n",
            "           8     1.0000    0.9808    0.9903       104\n",
            "\n",
            "    accuracy                         0.9437       941\n",
            "   macro avg     0.9309    0.9331    0.9317       941\n",
            "weighted avg     0.9447    0.9437    0.9440       941\n",
            "\n",
            "Test Loss: 0.7621, Test Acc: 0.7898, Test F1: 0.7683\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7183    0.8010    0.7574       191\n",
            "           1     0.9059    0.8370    0.8701        92\n",
            "           2     0.7333    0.6471    0.6875        51\n",
            "           3     0.8864    0.9070    0.8966        43\n",
            "           4     0.5000    0.3636    0.4211        33\n",
            "           5     0.8750    0.8305    0.8522        59\n",
            "           6     0.8947    0.7907    0.8395        43\n",
            "           7     0.6122    0.6383    0.6250        47\n",
            "           8     0.9324    1.0000    0.9650        69\n",
            "\n",
            "    accuracy                         0.7898       628\n",
            "   macro avg     0.7843    0.7572    0.7683       628\n",
            "weighted avg     0.7894    0.7898    0.7875       628\n",
            "\n",
            "\n",
            "Epoch [11/20]\n",
            "Train Loss: 0.1214, Train Acc: 0.9607, Train F1: 0.9551\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9577    0.9477    0.9527       287\n",
            "           1     1.0000    0.9928    0.9964       138\n",
            "           2     0.9200    0.9200    0.9200        75\n",
            "           3     0.9846    1.0000    0.9922        64\n",
            "           4     0.8269    0.8600    0.8431        50\n",
            "           5     0.9773    0.9885    0.9829        87\n",
            "           6     0.9692    0.9692    0.9692        65\n",
            "           7     0.9452    0.9718    0.9583        71\n",
            "           8     0.9902    0.9712    0.9806       104\n",
            "\n",
            "    accuracy                         0.9607       941\n",
            "   macro avg     0.9524    0.9579    0.9551       941\n",
            "weighted avg     0.9610    0.9607    0.9608       941\n",
            "\n",
            "Test Loss: 0.8007, Test Acc: 0.7818, Test F1: 0.7741\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7540    0.7382    0.7460       191\n",
            "           1     0.8636    0.8261    0.8444        92\n",
            "           2     0.7255    0.7255    0.7255        51\n",
            "           3     0.9250    0.8605    0.8916        43\n",
            "           4     0.4894    0.6970    0.5750        33\n",
            "           5     0.8136    0.8136    0.8136        59\n",
            "           6     0.7037    0.8837    0.7835        43\n",
            "           7     0.8621    0.5319    0.6579        47\n",
            "           8     0.9041    0.9565    0.9296        69\n",
            "\n",
            "    accuracy                         0.7818       628\n",
            "   macro avg     0.7823    0.7814    0.7741       628\n",
            "weighted avg     0.7923    0.7818    0.7822       628\n",
            "\n",
            "\n",
            "Epoch [12/20]\n",
            "Train Loss: 0.1599, Train Acc: 0.9437, Train F1: 0.9414\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9278    0.9408    0.9343       287\n",
            "           1     0.9716    0.9928    0.9821       138\n",
            "           2     0.9577    0.9067    0.9315        75\n",
            "           3     1.0000    0.9844    0.9921        64\n",
            "           4     0.8776    0.8600    0.8687        50\n",
            "           5     0.8989    0.9195    0.9091        87\n",
            "           6     0.9254    0.9538    0.9394        65\n",
            "           7     0.9853    0.9437    0.9640        71\n",
            "           8     0.9608    0.9423    0.9515       104\n",
            "\n",
            "    accuracy                         0.9437       941\n",
            "   macro avg     0.9450    0.9382    0.9414       941\n",
            "weighted avg     0.9440    0.9437    0.9437       941\n",
            "\n",
            "Test Loss: 0.9581, Test Acc: 0.7404, Test F1: 0.7239\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6367    0.8168    0.7156       191\n",
            "           1     0.8235    0.7609    0.7910        92\n",
            "           2     0.7674    0.6471    0.7021        51\n",
            "           3     0.7917    0.8837    0.8352        43\n",
            "           4     0.5143    0.5455    0.5294        33\n",
            "           5     0.9231    0.4068    0.5647        59\n",
            "           6     0.8684    0.7674    0.8148        43\n",
            "           7     0.8000    0.5106    0.6234        47\n",
            "           8     0.8846    1.0000    0.9388        69\n",
            "\n",
            "    accuracy                         0.7404       628\n",
            "   macro avg     0.7789    0.7043    0.7239       628\n",
            "weighted avg     0.7611    0.7404    0.7342       628\n",
            "\n",
            "\n",
            "Epoch [13/20]\n",
            "Train Loss: 0.0805, Train Acc: 0.9745, Train F1: 0.9746\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9649    0.9582    0.9615       287\n",
            "           1     1.0000    1.0000    1.0000       138\n",
            "           2     0.9737    0.9867    0.9801        75\n",
            "           3     0.9846    1.0000    0.9922        64\n",
            "           4     0.9038    0.9400    0.9216        50\n",
            "           5     0.9765    0.9540    0.9651        87\n",
            "           6     0.9697    0.9846    0.9771        65\n",
            "           7     0.9861    1.0000    0.9930        71\n",
            "           8     0.9902    0.9712    0.9806       104\n",
            "\n",
            "    accuracy                         0.9745       941\n",
            "   macro avg     0.9722    0.9772    0.9746       941\n",
            "weighted avg     0.9746    0.9745    0.9745       941\n",
            "\n",
            "Test Loss: 0.9774, Test Acc: 0.7596, Test F1: 0.7118\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6639    0.8272    0.7366       191\n",
            "           1     0.8202    0.7935    0.8066        92\n",
            "           2     0.8000    0.6275    0.7033        51\n",
            "           3     0.8298    0.9070    0.8667        43\n",
            "           4     0.8000    0.1212    0.2105        33\n",
            "           5     0.6463    0.8983    0.7518        59\n",
            "           6     0.8529    0.6744    0.7532        43\n",
            "           7     0.8750    0.4468    0.5915        47\n",
            "           8     0.9855    0.9855    0.9855        69\n",
            "\n",
            "    accuracy                         0.7596       628\n",
            "   macro avg     0.8082    0.6979    0.7118       628\n",
            "weighted avg     0.7788    0.7596    0.7445       628\n",
            "\n",
            "\n",
            "Epoch [14/20]\n",
            "Train Loss: 0.0672, Train Acc: 0.9787, Train F1: 0.9777\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9655    0.9756    0.9705       287\n",
            "           1     1.0000    0.9928    0.9964       138\n",
            "           2     1.0000    0.9733    0.9865        75\n",
            "           3     1.0000    1.0000    1.0000        64\n",
            "           4     0.9216    0.9400    0.9307        50\n",
            "           5     0.9775    1.0000    0.9886        87\n",
            "           6     0.9697    0.9846    0.9771        65\n",
            "           7     0.9714    0.9577    0.9645        71\n",
            "           8     1.0000    0.9712    0.9854       104\n",
            "\n",
            "    accuracy                         0.9787       941\n",
            "   macro avg     0.9784    0.9772    0.9777       941\n",
            "weighted avg     0.9790    0.9787    0.9788       941\n",
            "\n",
            "Test Loss: 0.8517, Test Acc: 0.7707, Test F1: 0.7409\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7037    0.7958    0.7469       191\n",
            "           1     0.8280    0.8370    0.8324        92\n",
            "           2     0.7949    0.6078    0.6889        51\n",
            "           3     0.8810    0.8605    0.8706        43\n",
            "           4     0.5625    0.2727    0.3673        33\n",
            "           5     0.8361    0.8644    0.8500        59\n",
            "           6     0.9310    0.6279    0.7500        43\n",
            "           7     0.5424    0.6809    0.6038        47\n",
            "           8     0.9315    0.9855    0.9577        69\n",
            "\n",
            "    accuracy                         0.7707       628\n",
            "   macro avg     0.7790    0.7258    0.7409       628\n",
            "weighted avg     0.7750    0.7707    0.7656       628\n",
            "\n",
            "\n",
            "Epoch [15/20]\n",
            "Train Loss: 0.0195, Train Acc: 0.9957, Train F1: 0.9958\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9930    0.9930    0.9930       287\n",
            "           1     1.0000    1.0000    1.0000       138\n",
            "           2     1.0000    0.9867    0.9933        75\n",
            "           3     1.0000    1.0000    1.0000        64\n",
            "           4     0.9804    1.0000    0.9901        50\n",
            "           5     1.0000    1.0000    1.0000        87\n",
            "           6     1.0000    1.0000    1.0000        65\n",
            "           7     0.9859    0.9859    0.9859        71\n",
            "           8     1.0000    1.0000    1.0000       104\n",
            "\n",
            "    accuracy                         0.9957       941\n",
            "   macro avg     0.9955    0.9962    0.9958       941\n",
            "weighted avg     0.9958    0.9957    0.9958       941\n",
            "\n",
            "Test Loss: 0.8731, Test Acc: 0.7818, Test F1: 0.7647\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7568    0.7330    0.7447       191\n",
            "           1     0.8617    0.8804    0.8710        92\n",
            "           2     0.7800    0.7647    0.7723        51\n",
            "           3     0.8780    0.8372    0.8571        43\n",
            "           4     0.4242    0.4242    0.4242        33\n",
            "           5     0.8305    0.8305    0.8305        59\n",
            "           6     0.8824    0.6977    0.7792        43\n",
            "           7     0.5397    0.7234    0.6182        47\n",
            "           8     0.9855    0.9855    0.9855        69\n",
            "\n",
            "    accuracy                         0.7818       628\n",
            "   macro avg     0.7710    0.7641    0.7647       628\n",
            "weighted avg     0.7893    0.7818    0.7837       628\n",
            "\n",
            "\n",
            "Epoch [16/20]\n",
            "Train Loss: 0.0139, Train Acc: 0.9979, Train F1: 0.9983\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9965    0.9965    0.9965       287\n",
            "           1     1.0000    1.0000    1.0000       138\n",
            "           2     1.0000    1.0000    1.0000        75\n",
            "           3     1.0000    1.0000    1.0000        64\n",
            "           4     1.0000    1.0000    1.0000        50\n",
            "           5     1.0000    1.0000    1.0000        87\n",
            "           6     1.0000    1.0000    1.0000        65\n",
            "           7     0.9861    1.0000    0.9930        71\n",
            "           8     1.0000    0.9904    0.9952       104\n",
            "\n",
            "    accuracy                         0.9979       941\n",
            "   macro avg     0.9981    0.9985    0.9983       941\n",
            "weighted avg     0.9979    0.9979    0.9979       941\n",
            "\n",
            "Test Loss: 0.8445, Test Acc: 0.7962, Test F1: 0.7763\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7376    0.7801    0.7583       191\n",
            "           1     0.8235    0.9130    0.8660        92\n",
            "           2     0.6909    0.7451    0.7170        51\n",
            "           3     0.9000    0.8372    0.8675        43\n",
            "           4     0.5200    0.3939    0.4483        33\n",
            "           5     0.9057    0.8136    0.8571        59\n",
            "           6     0.8750    0.8140    0.8434        43\n",
            "           7     0.7000    0.5957    0.6437        47\n",
            "           8     0.9718    1.0000    0.9857        69\n",
            "\n",
            "    accuracy                         0.7962       628\n",
            "   macro avg     0.7916    0.7659    0.7763       628\n",
            "weighted avg     0.7942    0.7962    0.7934       628\n",
            "\n",
            "\n",
            "Epoch [17/20]\n",
            "Train Loss: 0.0074, Train Acc: 1.0000, Train F1: 1.0000\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    1.0000    1.0000       287\n",
            "           1     1.0000    1.0000    1.0000       138\n",
            "           2     1.0000    1.0000    1.0000        75\n",
            "           3     1.0000    1.0000    1.0000        64\n",
            "           4     1.0000    1.0000    1.0000        50\n",
            "           5     1.0000    1.0000    1.0000        87\n",
            "           6     1.0000    1.0000    1.0000        65\n",
            "           7     1.0000    1.0000    1.0000        71\n",
            "           8     1.0000    1.0000    1.0000       104\n",
            "\n",
            "    accuracy                         1.0000       941\n",
            "   macro avg     1.0000    1.0000    1.0000       941\n",
            "weighted avg     1.0000    1.0000    1.0000       941\n",
            "\n",
            "Test Loss: 0.9400, Test Acc: 0.7882, Test F1: 0.7607\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6978    0.8220    0.7548       191\n",
            "           1     0.8265    0.8804    0.8526        92\n",
            "           2     0.8205    0.6275    0.7111        51\n",
            "           3     0.9024    0.8605    0.8810        43\n",
            "           4     0.5000    0.3030    0.3774        33\n",
            "           5     0.8772    0.8475    0.8621        59\n",
            "           6     0.8611    0.7209    0.7848        43\n",
            "           7     0.7000    0.5957    0.6437        47\n",
            "           8     0.9583    1.0000    0.9787        69\n",
            "\n",
            "    accuracy                         0.7882       628\n",
            "   macro avg     0.7938    0.7397    0.7607       628\n",
            "weighted avg     0.7871    0.7882    0.7828       628\n",
            "\n",
            "\n",
            "Epoch [18/20]\n",
            "Train Loss: 0.0055, Train Acc: 1.0000, Train F1: 1.0000\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    1.0000    1.0000       287\n",
            "           1     1.0000    1.0000    1.0000       138\n",
            "           2     1.0000    1.0000    1.0000        75\n",
            "           3     1.0000    1.0000    1.0000        64\n",
            "           4     1.0000    1.0000    1.0000        50\n",
            "           5     1.0000    1.0000    1.0000        87\n",
            "           6     1.0000    1.0000    1.0000        65\n",
            "           7     1.0000    1.0000    1.0000        71\n",
            "           8     1.0000    1.0000    1.0000       104\n",
            "\n",
            "    accuracy                         1.0000       941\n",
            "   macro avg     1.0000    1.0000    1.0000       941\n",
            "weighted avg     1.0000    1.0000    1.0000       941\n",
            "\n",
            "Test Loss: 0.8606, Test Acc: 0.7978, Test F1: 0.7768\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7549    0.8063    0.7797       191\n",
            "           1     0.8526    0.8804    0.8663        92\n",
            "           2     0.7727    0.6667    0.7158        51\n",
            "           3     0.8810    0.8605    0.8706        43\n",
            "           4     0.4688    0.4545    0.4615        33\n",
            "           5     0.8167    0.8305    0.8235        59\n",
            "           6     0.8718    0.7907    0.8293        43\n",
            "           7     0.7045    0.6596    0.6813        47\n",
            "           8     0.9706    0.9565    0.9635        69\n",
            "\n",
            "    accuracy                         0.7978       628\n",
            "   macro avg     0.7882    0.7673    0.7768       628\n",
            "weighted avg     0.7980    0.7978    0.7971       628\n",
            "\n",
            "\n",
            "Epoch [19/20]\n",
            "Train Loss: 0.0098, Train Acc: 0.9979, Train F1: 0.9983\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9965    0.9965    0.9965       287\n",
            "           1     1.0000    1.0000    1.0000       138\n",
            "           2     0.9868    1.0000    0.9934        75\n",
            "           3     1.0000    1.0000    1.0000        64\n",
            "           4     1.0000    1.0000    1.0000        50\n",
            "           5     1.0000    1.0000    1.0000        87\n",
            "           6     1.0000    1.0000    1.0000        65\n",
            "           7     1.0000    1.0000    1.0000        71\n",
            "           8     1.0000    0.9904    0.9952       104\n",
            "\n",
            "    accuracy                         0.9979       941\n",
            "   macro avg     0.9982    0.9985    0.9983       941\n",
            "weighted avg     0.9979    0.9979    0.9979       941\n",
            "\n",
            "Test Loss: 0.8945, Test Acc: 0.7755, Test F1: 0.7563\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7374    0.7644    0.7506       191\n",
            "           1     0.8539    0.8261    0.8398        92\n",
            "           2     0.7442    0.6275    0.6809        51\n",
            "           3     0.8605    0.8605    0.8605        43\n",
            "           4     0.4054    0.4545    0.4286        33\n",
            "           5     0.7846    0.8644    0.8226        59\n",
            "           6     0.8684    0.7674    0.8148        43\n",
            "           7     0.6591    0.6170    0.6374        47\n",
            "           8     0.9577    0.9855    0.9714        69\n",
            "\n",
            "    accuracy                         0.7755       628\n",
            "   macro avg     0.7635    0.7519    0.7563       628\n",
            "weighted avg     0.7778    0.7755    0.7756       628\n",
            "\n",
            "\n",
            "Epoch [20/20]\n",
            "Train Loss: 0.0055, Train Acc: 0.9989, Train F1: 0.9987\n",
            "Train classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    1.0000    1.0000       287\n",
            "           1     1.0000    1.0000    1.0000       138\n",
            "           2     1.0000    1.0000    1.0000        75\n",
            "           3     1.0000    1.0000    1.0000        64\n",
            "           4     1.0000    1.0000    1.0000        50\n",
            "           5     1.0000    1.0000    1.0000        87\n",
            "           6     1.0000    1.0000    1.0000        65\n",
            "           7     0.9861    1.0000    0.9930        71\n",
            "           8     1.0000    0.9904    0.9952       104\n",
            "\n",
            "    accuracy                         0.9989       941\n",
            "   macro avg     0.9985    0.9989    0.9987       941\n",
            "weighted avg     0.9990    0.9989    0.9989       941\n",
            "\n",
            "Test Loss: 0.8997, Test Acc: 0.7850, Test F1: 0.7624\n",
            "Test classification report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7317    0.7853    0.7576       191\n",
            "           1     0.8587    0.8587    0.8587        92\n",
            "           2     0.8049    0.6471    0.7174        51\n",
            "           3     0.9048    0.8837    0.8941        43\n",
            "           4     0.3714    0.3939    0.3824        33\n",
            "           5     0.8095    0.8644    0.8361        59\n",
            "           6     0.8611    0.7209    0.7848        43\n",
            "           7     0.6905    0.6170    0.6517        47\n",
            "           8     0.9583    1.0000    0.9787        69\n",
            "\n",
            "    accuracy                         0.7850       628\n",
            "   macro avg     0.7768    0.7523    0.7624       628\n",
            "weighted avg     0.7872    0.7850    0.7844       628\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN FULL"
      ],
      "metadata": {
        "id": "w79PBROlP3jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "aBYV7dGSGk2z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_loader = DataLoader(full_data_background, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "S75VuoI-FNu_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "model = models.vit_b_16(pretrained=True)\n",
        "\n",
        "model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
        "\n",
        "# Utilisation du GPU si disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "_OC5JxCRMP1z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\n",
        "                                                       factor=0.5, patience=3, verbose=True)"
      ],
      "metadata": {
        "id": "MiQXjaWEMmn2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_train(model_vit, full_loader, criterion, optimizer, EPOCHS=50):\n",
        "    best_f1 = 0.0  # Meilleur F1 score\n",
        "    best_model_wts = None\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model_vit.train()  # Passer en mode entraînement\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_labels = []\n",
        "        all_predictions = []\n",
        "\n",
        "        epoch_iterator = tqdm(full_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\")\n",
        "\n",
        "        for inputs, labels in epoch_iterator:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model_vit(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # Calcul des métriques\n",
        "        epoch_loss = running_loss / len(full_loader)\n",
        "        epoch_acc = 100 * correct / total\n",
        "        epoch_f1 = f1_score(all_labels, all_predictions, average=\"macro\")\n",
        "\n",
        "        # Affichage des métriques\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {epoch_loss:.4f} - Acc: {epoch_acc:.4f} | F1 Score: {epoch_f1:.4f}\")\n",
        "\n",
        "        # Si le F1 score est le meilleur, sauvegarder le modèle\n",
        "        if epoch_f1 > best_f1:\n",
        "            best_f1 = epoch_f1\n",
        "            best_model_wts = model_vit.state_dict()\n",
        "\n",
        "        # Mettre à jour le scheduler\n",
        "        scheduler.step(epoch_f1)\n",
        "\n",
        "    # Sauvegarde du meilleur modèle basé sur F1 score\n",
        "    model_vit.load_state_dict(best_model_wts)\n",
        "    run_name = f\"VITAXEL_NET_MIN3_NEWLOSS_{BATCH_SIZE}BATCH_{LR}LR_{EPOCHS}EPOCH_{image_size}SIZE\"\n",
        "    torch.save(model_vit.state_dict(), f\"{run_name}.pth\")\n",
        "    print(f\"Training complete. Best F1 Score: {best_f1:.4f}\")\n",
        "\n",
        "full_train(model, full_loader, criterion, optimizer, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxpAd7RxMzDl",
        "outputId": "63ffdc73-d2e7-4848-cbfc-8c27a0170c38"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50: 100%|██████████| 197/197 [07:51<00:00,  2.39s/batch]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 - Loss: 1.4206 - Acc: 50.2231 | F1 Score: 0.3940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50 - Loss: 0.9807 - Acc: 63.7986 | F1 Score: 0.5898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50: 100%|██████████| 197/197 [01:23<00:00,  2.35batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50 - Loss: 0.8049 - Acc: 71.5743 | F1 Score: 0.6748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50: 100%|██████████| 197/197 [01:24<00:00,  2.32batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/50 - Loss: 0.6328 - Acc: 77.2467 | F1 Score: 0.7442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50: 100%|██████████| 197/197 [01:22<00:00,  2.38batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50 - Loss: 0.5720 - Acc: 79.2862 | F1 Score: 0.7664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50: 100%|██████████| 197/197 [01:21<00:00,  2.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50 - Loss: 0.5072 - Acc: 80.9433 | F1 Score: 0.7878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50 - Loss: 0.4518 - Acc: 83.8113 | F1 Score: 0.8191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50: 100%|██████████| 197/197 [01:23<00:00,  2.37batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/50 - Loss: 0.4126 - Acc: 85.2772 | F1 Score: 0.8389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50: 100%|██████████| 197/197 [01:22<00:00,  2.39batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/50 - Loss: 0.4157 - Acc: 84.3212 | F1 Score: 0.8209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50 - Loss: 0.3065 - Acc: 89.0376 | F1 Score: 0.8786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50: 100%|██████████| 197/197 [01:21<00:00,  2.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50 - Loss: 0.3133 - Acc: 88.8464 | F1 Score: 0.8783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50 - Loss: 0.2431 - Acc: 91.5870 | F1 Score: 0.9048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50: 100%|██████████| 197/197 [01:21<00:00,  2.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50 - Loss: 0.2212 - Acc: 91.8419 | F1 Score: 0.9083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50 - Loss: 0.2509 - Acc: 91.5233 | F1 Score: 0.9058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50: 100%|██████████| 197/197 [01:21<00:00,  2.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50 - Loss: 0.2071 - Acc: 92.4156 | F1 Score: 0.9163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50: 100%|██████████| 197/197 [01:22<00:00,  2.38batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50 - Loss: 0.2577 - Acc: 91.5233 | F1 Score: 0.9100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50: 100%|██████████| 197/197 [01:22<00:00,  2.39batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50 - Loss: 0.2615 - Acc: 90.3123 | F1 Score: 0.8978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50: 100%|██████████| 197/197 [01:22<00:00,  2.39batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50 - Loss: 0.1749 - Acc: 94.5188 | F1 Score: 0.9427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50 - Loss: 0.1377 - Acc: 95.4111 | F1 Score: 0.9484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50: 100%|██████████| 197/197 [01:21<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50 - Loss: 0.1342 - Acc: 95.5386 | F1 Score: 0.9515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50 - Loss: 0.1317 - Acc: 95.3474 | F1 Score: 0.9516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50 - Loss: 0.1755 - Acc: 94.2639 | F1 Score: 0.9351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50 - Loss: 0.1250 - Acc: 95.9210 | F1 Score: 0.9567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50: 100%|██████████| 197/197 [01:21<00:00,  2.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50 - Loss: 0.0995 - Acc: 96.7495 | F1 Score: 0.9662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50 - Loss: 0.1526 - Acc: 95.0287 | F1 Score: 0.9447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50: 100%|██████████| 197/197 [01:21<00:00,  2.42batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50 - Loss: 0.1409 - Acc: 94.7737 | F1 Score: 0.9449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50: 100%|██████████| 197/197 [01:22<00:00,  2.39batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50 - Loss: 0.1355 - Acc: 95.3474 | F1 Score: 0.9515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50: 100%|██████████| 197/197 [01:22<00:00,  2.38batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50 - Loss: 0.1319 - Acc: 95.5386 | F1 Score: 0.9544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50: 100%|██████████| 197/197 [01:21<00:00,  2.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50 - Loss: 0.0535 - Acc: 98.4704 | F1 Score: 0.9839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50: 100%|██████████| 197/197 [01:21<00:00,  2.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50 - Loss: 0.0201 - Acc: 99.3627 | F1 Score: 0.9929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50: 100%|██████████| 197/197 [01:21<00:00,  2.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50 - Loss: 0.0104 - Acc: 99.8088 | F1 Score: 0.9984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50: 100%|██████████| 197/197 [01:21<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50 - Loss: 0.0169 - Acc: 99.6813 | F1 Score: 0.9964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50: 100%|██████████| 197/197 [01:21<00:00,  2.42batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50 - Loss: 0.0122 - Acc: 99.7451 | F1 Score: 0.9980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50: 100%|██████████| 197/197 [01:20<00:00,  2.43batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50 - Loss: 0.0194 - Acc: 99.5539 | F1 Score: 0.9954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50: 100%|██████████| 197/197 [01:21<00:00,  2.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50 - Loss: 0.0142 - Acc: 99.6176 | F1 Score: 0.9948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50: 100%|██████████| 197/197 [01:21<00:00,  2.42batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50 - Loss: 0.0103 - Acc: 99.6176 | F1 Score: 0.9962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50 - Loss: 0.0029 - Acc: 99.9363 | F1 Score: 0.9994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50 - Loss: 0.0023 - Acc: 99.9363 | F1 Score: 0.9994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50 - Loss: 0.0018 - Acc: 100.0000 | F1 Score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50: 100%|██████████| 197/197 [01:21<00:00,  2.42batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50 - Loss: 0.0008 - Acc: 100.0000 | F1 Score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50: 100%|██████████| 197/197 [01:21<00:00,  2.42batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50 - Loss: 0.0014 - Acc: 99.9363 | F1 Score: 0.9996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50: 100%|██████████| 197/197 [01:21<00:00,  2.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50 - Loss: 0.0013 - Acc: 99.9363 | F1 Score: 0.9992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50: 100%|██████████| 197/197 [01:21<00:00,  2.42batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50 - Loss: 0.0056 - Acc: 99.8088 | F1 Score: 0.9974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50: 100%|██████████| 197/197 [01:21<00:00,  2.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50 - Loss: 0.0010 - Acc: 100.0000 | F1 Score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50: 100%|██████████| 197/197 [01:21<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50 - Loss: 0.0007 - Acc: 100.0000 | F1 Score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50: 100%|██████████| 197/197 [01:22<00:00,  2.40batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50 - Loss: 0.0006 - Acc: 100.0000 | F1 Score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50: 100%|██████████| 197/197 [01:21<00:00,  2.41batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50 - Loss: 0.0005 - Acc: 100.0000 | F1 Score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50: 100%|██████████| 197/197 [01:22<00:00,  2.39batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50 - Loss: 0.0006 - Acc: 100.0000 | F1 Score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50: 100%|██████████| 197/197 [01:22<00:00,  2.39batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50 - Loss: 0.0010 - Acc: 100.0000 | F1 Score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50: 100%|██████████| 197/197 [01:22<00:00,  2.38batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50 - Loss: 0.0008 - Acc: 100.0000 | F1 Score: 1.0000\n",
            "Training complete. Best F1 Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_name = f\"VIT_MIN4_NEWLABEL_CNNVIT_{BATCH_SIZE}BATCH_{LR}LR_{EPOCHS}EPOCH_{image_size}SIZE\"\n",
        "torch.save(model.state_dict(), f\"{run_name}.pth\")"
      ],
      "metadata": {
        "id": "lYI2JldjjHMP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "cfW8asfRP9CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "w562oWRKlWp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd72100-f73d-4a4b-c86c-3dbc87ece6ee"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): Sequential(\n",
              "      (encoder_layer_0): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_1): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_2): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_3): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_4): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_5): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_6): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_7): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_8): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_9): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_10): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_11): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (heads): Sequential(\n",
              "    (head): Linear(in_features=768, out_features=9, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "model = models.vit_b_16(pretrained=True)\n",
        "\n",
        "model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
        "# à changer\n",
        "model.to(device)\n",
        "run_name = \"/content/drive/MyDrive/projet_collembolles/VIT_MIN4_NEWLABEL_CNNVIT_8BATCH_0.0001LR_50EPOCH_224SIZE.pth\"\n",
        "model.load_state_dict(torch.load(f'{run_name}'))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "hcGZZiaOP8Yh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a44b7fbb-c524-45c0-bfdf-75020f937c19"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): Sequential(\n",
              "      (encoder_layer_0): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_1): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_2): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_3): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_4): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_5): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_6): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_7): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_8): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_9): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_10): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_11): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (heads): Sequential(\n",
              "    (head): Linear(in_features=768, out_features=9, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTestDataset(Dataset):\n",
        "    def __init__(self, images_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.image_files[idx]\n",
        "        image_path = os.path.join(self.images_dir, image_name)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        image_name = os.path.splitext(image_name)[0]\n",
        "\n",
        "        return image, image_name"
      ],
      "metadata": {
        "id": "7Q-aN4s4P8Wj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size), antialias=True),\n",
        "    transforms.ToTensor(),\n",
        "    ])"
      ],
      "metadata": {
        "id": "ME696hUpP8Ul"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_img_dir = '/content/drive/MyDrive/projet_collembolles/datatest'\n",
        "\n",
        "# Créer un Dataset pour le test\n",
        "test_dataset = CustomTestDataset(images_dir=test_img_dir, transform=transform)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "print(f\"Nombre d'images dans le DataLoader de test: {len(test_loader.dataset)}\")"
      ],
      "metadata": {
        "id": "LKxP1kSAP8SB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f966fba-b47c-46a3-afb5-2c556af35945"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre d'images dans le DataLoader de test: 1344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "results = []\n",
        "\n",
        "# Assurer que le modèle est en mode évaluation\n",
        "model.eval()\n",
        "\n",
        "# Désactiver la mise à jour des gradients\n",
        "with torch.no_grad():\n",
        "    for idx, (inputs, image_names) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Prédictions du modèle\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Ajouter au résultat (nom de l'image sans extension et la classe prédite)\n",
        "        for i in range(len(inputs)):\n",
        "            # On s'assure que le nom de l'image est correctement formaté sans tuple\n",
        "            image_name = image_names[i]  # Récupérer le nom du fichier de l'image\n",
        "\n",
        "            # Ajouter les résultats\n",
        "            predicted_class = predicted[i].item()\n",
        "            results.append([image_name, predicted_class])\n",
        "\n",
        "# Sauvegarder les résultats dans un fichier CSV\n",
        "csv_output_path = '/content/drive/MyDrive/projet_collembolles/VIT_MIN4_NEWLABEL_CNNVIT.csv'\n",
        "with open(csv_output_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['idx', 'gt'])  # Écrire l'en-tête\n",
        "    writer.writerows(results)  # Écrire les lignes des résultats\n",
        "\n",
        "print(f\"Fichier CSV de soumission sauvegardé sous {csv_output_path}\")\n"
      ],
      "metadata": {
        "id": "qUCHgRWaT1EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4118f1c4-90c6-4c0c-dacc-4e9cb1558c9f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 168/168 [00:41<00:00,  4.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fichier CSV de soumission sauvegardé sous /content/drive/MyDrive/projet_collembolles/VIT_MIN4_NEWLABEL_CNNVIT.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "results = []\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Assurer que le modèle est en mode évaluation\n",
        "model.eval()\n",
        "\n",
        "# Désactiver la mise à jour des gradients\n",
        "with torch.no_grad():\n",
        "    for idx, (inputs, image_names) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Prédictions du modèle\n",
        "        outputs = model(inputs)\n",
        "        probas_axel = F.softmax(outputs, dim=1)\n",
        "\n",
        "        # Ajouter au résultat (nom de l'image sans extension et la classe prédite)\n",
        "        for i in range(len(inputs)):\n",
        "            # On s'assure que le nom de l'image est correctement formaté sans tuple\n",
        "            image_name = image_names[i]  # Récupérer le nom du fichier de l'image\n",
        "\n",
        "            # Ajouter les résultats\n",
        "            predicted = probas_axel[i].tolist()\n",
        "            results.append([image_name, predicted])\n",
        "\n",
        "# Sauvegarder les résultats dans un fichier CSV\n",
        "csv_output_path = '/content/drive/MyDrive/projet_collembolles/VIT_MIN4_NEWLABEL_CNNVIT_softmax.csv'\n",
        "with open(csv_output_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['idx', 'gt'])  # Écrire l'en-tête\n",
        "    writer.writerows(results)  # Écrire les lignes des résultats\n",
        "\n",
        "print(f\"Fichier CSV de soumission sauvegardé sous {csv_output_path}\")"
      ],
      "metadata": {
        "id": "v52uPa44T0_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf6268e-086c-499e-8c5b-7a373bb4d0b6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 168/168 [00:17<00:00,  9.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fichier CSV de soumission sauvegardé sous /content/drive/MyDrive/projet_collembolles/VIT_MIN4_NEWLABEL_CNNVIT_softmax.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tHYk4XnaT09t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nI3LF9j3T07d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJaH2FrGT05S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jpLb3JPaP8Pc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}